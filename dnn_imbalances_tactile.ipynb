{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, f1_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data \n",
    "#algorithm to read all the files\n",
    "\n",
    "'''\n",
    "for folder in this folder:\n",
    "    read xelasensor1.csv\n",
    "    read sliplabel.csv\n",
    "    concat it in a single dataframe along axis = 0\n",
    "\n",
    "print the dataframe\n",
    "'''\n",
    "import os\n",
    "\n",
    "directory = 'CNN-GradCAM/train2dof'\n",
    "directory2 = 'train2dof'\n",
    "\n",
    "def read_file(detect_or_pred, n = None):\n",
    "\n",
    "    #store all directories in a list\n",
    "    list_xela_allfiles = []\n",
    "    list_sliplabel_allfiles = []\n",
    "\n",
    "    for root, subdirectories, files in os.walk(directory):\n",
    "        for sdirectory in subdirectories:\n",
    "\n",
    "            #subdirectory with absolute path\n",
    "            subdirectory = '{}/{}'.format(root, sdirectory)\n",
    "\n",
    "            #read specific files in the subdirectory\n",
    "            for file in os.listdir(subdirectory):\n",
    "            \n",
    "                if file.endswith(\"sensor1.csv\"):\n",
    "                    df = pd.read_csv('{}/{}'.format(subdirectory, file), index_col=None, header=0)\n",
    "                    \n",
    "                    if detect_or_pred ==0:\n",
    "                        list_xela_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None:\n",
    "                        list_xela_allfiles.append(df[:-n])\n",
    "\n",
    "                if file.endswith(\"label.csv\"):\n",
    "                    df = pd.read_csv('{}/{}'.format(subdirectory, file), index_col=None, header=0)\n",
    "                    if detect_or_pred ==0:\n",
    "                        list_sliplabel_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None: \n",
    "                        list_sliplabel_allfiles.append(df[n:])\n",
    "\n",
    "    return list_xela_allfiles, list_sliplabel_allfiles\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRICS DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detection_metrics(xela_test, sliplabel_test, model_type, acc = None):\n",
    "    #predict using the holdout set (DONE)\n",
    "    predicted = model_type(xela_test).detach().numpy()\n",
    "    predicted_cls = predicted.round()\n",
    "\n",
    "    #Plot the loss values against number of epochs (DONE)\n",
    "    #validation test (DONE)\n",
    "\n",
    "    #Print the accuracy\n",
    "    accuracy = accuracy_score(sliplabel_test.numpy(), predicted_cls)\n",
    "    print(f'Accuracy for slip detection is {accuracy}')\n",
    "\n",
    "    #Print the fscore\n",
    "    fscore = f1_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Fscore for slip detection is {fscore}')\n",
    "\n",
    "    #print the Precision\n",
    "    precision = precision_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Precision for slip detection is {precision}')\n",
    "\n",
    "    #print the Recall\n",
    "    recall = recall_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Recall for slip detection is {recall}')\n",
    "\n",
    "def slip_metrics(n, xela_test, sliplabel_test, modeltype):\n",
    "    #predict using the holdout set (DONE)\n",
    "    predicted = modeltype(xela_test).detach().numpy()\n",
    "    predicted_cls = predicted.round()\n",
    "\n",
    "    #Plot the loss values against number of epochs (DONE)\n",
    "    #validation test (DONE)\n",
    "\n",
    "    #Print the accuracy\n",
    "    x = 0\n",
    "    for i in range(predicted_cls.shape[0]):\n",
    "        if predicted_cls[i].item() == sliplabel_test[i].item():\n",
    "            x += 1\n",
    "\n",
    "    accuracy = x/ float(sliplabel_test.shape[0])\n",
    "    print(f'Accuracy for slip prediction for (t+{n}) is {accuracy}')\n",
    "\n",
    "    #Print the fscore\n",
    "    fscore = f1_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Fscore for slip prediction for (t+{n}) is {fscore}')\n",
    "\n",
    "    #print the Precision\n",
    "    precision = precision_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Precision for slip prediction for (t+{n}) is {precision}')\n",
    "\n",
    "    #print the Recall\n",
    "    recall = recall_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Recall for slip prediction for (t+{n}) is {recall}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIP DETECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the list of xela_allfiles and sliplabel_allfiles across axis = 0\n",
    "list_xela_allfiles, list_sliplabel_allfiles = read_file(0)\n",
    "pd_xela_allfiles = pd.concat(list_xela_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd.concat(list_sliplabel_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd_sliplabel_allfiles['slip']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dealing with imbalanced data using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "# Fit the model to generate the data.\n",
    "pd_xela_allfiles, pd_sliplabel_allfiles = sm.fit_resample(pd_xela_allfiles, pd_sliplabel_allfiles)\n",
    "oversampled = pd.concat([pd.DataFrame(pd_xela_allfiles), pd.DataFrame(pd_sliplabel_allfiles)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy values\n",
    "pd_xela_allfiles = np.array(pd_xela_allfiles.values)\n",
    "pd_sliplabel_allfiles = np.array(pd_sliplabel_allfiles.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test\n",
    "xela_train, xela_test, sliplabel_train, sliplabel_test = train_test_split(pd_xela_allfiles, pd_sliplabel_allfiles, shuffle=True, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into validation and holdout\n",
    "xela_train, xela_valid, sliplabel_train, sliplabel_valid = train_test_split(xela_train, sliplabel_train, shuffle = True, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114802, 48)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xela_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "sc = StandardScaler()\n",
    "xela_train = sc.fit_transform(xela_train)\n",
    "xela_test = sc.transform(xela_test)\n",
    "xela_valid = sc.transform(xela_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tensor values\n",
    "xela_train = torch.from_numpy(xela_train.astype(np.float32))\n",
    "xela_test = torch.from_numpy(xela_test.astype(np.float32))\n",
    "\n",
    "sliplabel_train = torch.from_numpy(sliplabel_train.astype(np.float32))\n",
    "sliplabel_test = torch.from_numpy(sliplabel_test.astype(np.float32))\n",
    "\n",
    "xela_valid = torch.from_numpy(xela_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "sliplabel_valid = torch.from_numpy(sliplabel_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "#reshape the y tensor\n",
    "sliplabel_train = sliplabel_train.view(sliplabel_train.shape[0], 1)\n",
    "sliplabel_test = sliplabel_test.view(sliplabel_test.shape[0], 1)\n",
    "\n",
    "sliplabel_valid = sliplabel_valid.view(sliplabel_valid.shape[0], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply batch training on the training/validation data\n",
    "class Batchdata(Dataset):\n",
    "    def __init__(self, xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = None):\n",
    "        self.x = xela_train\n",
    "        self.y = sliplabel_train\n",
    "        self.xvalid = xela_valid\n",
    "        self.yvalid = sliplabel_valid\n",
    "        self.valid = valid\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.valid == True:\n",
    "            return self.xvalid.shape[0]\n",
    "        else:\n",
    "            return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.valid == True:\n",
    "            return self.xvalid[idx], self.yvalid[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "dataset = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid)\n",
    "dataset2 = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = True)\n",
    "\n",
    "xelaloader = DataLoader(dataset = dataset, batch_size=32, shuffle=True)\n",
    "xelaloadervalid = DataLoader(dataset = dataset2, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the NN class\n",
    "\n",
    "inputsize = xela_train.shape[1]\n",
    "outputsize = 1\n",
    "hiddensize1 = 16\n",
    "hiddensize2 = 10\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize1, hiddensize2, outputsize):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(inputsize, hiddensize1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hiddensize1, hiddensize2)\n",
    "        self.l3 = nn.Linear(hiddensize2, outputsize)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet(inputsize, hiddensize1, hiddensize2, outputsize).to('cpu')\n",
    "model.load_state_dict(torch.load('models/DNN.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=48, out_features=16, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (l2): Linear(in_features=16, out_features=10, bias=True)\n",
       "  (l3): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = sliplabel_valid\n",
    "model.eval()  # set model to evaluation mode\n",
    "with torch.no_grad():  # disable gradient computation\n",
    "    y_pred = model(xela_valid)\n",
    "    y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "class_names = [\"No-slip\", \"Slip\"]  # 0 → No-slip, 1 → Slip\n",
    "\n",
    "# 3. Display with labels and title\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "# 4. Plot \n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "disp.plot(cmap='Blues', ax=ax)\n",
    "plt.title(\"Confusion Matrix: SMOTE-CNN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training epoch 1, loss =0.12242735 For validation epoch 1, loss =0.11427331\n",
      "For training epoch 2, loss =0.07393983 For validation epoch 2, loss =0.01158634\n",
      "For training epoch 3, loss =0.13840209 For validation epoch 3, loss =0.02297569\n",
      "For training epoch 4, loss =0.09802894 For validation epoch 4, loss =0.14555718\n",
      "For training epoch 5, loss =0.00711670 For validation epoch 5, loss =0.01778943\n",
      "For training epoch 6, loss =0.00263717 For validation epoch 6, loss =0.02079939\n",
      "For training epoch 7, loss =0.01130870 For validation epoch 7, loss =0.00609502\n",
      "For training epoch 8, loss =0.01599028 For validation epoch 8, loss =0.08565203\n",
      "For training epoch 9, loss =0.22539422 For validation epoch 9, loss =0.03270976\n",
      "For training epoch 10, loss =0.06984029 For validation epoch 10, loss =0.03342824\n",
      "For training epoch 11, loss =0.00904876 For validation epoch 11, loss =0.00109306\n",
      "For training epoch 12, loss =0.02612330 For validation epoch 12, loss =0.00412355\n",
      "For training epoch 13, loss =0.00196989 For validation epoch 13, loss =0.00542692\n",
      "For training epoch 14, loss =0.08675880 For validation epoch 14, loss =0.12457057\n",
      "For training epoch 15, loss =0.00233277 For validation epoch 15, loss =0.00287283\n",
      "For training epoch 16, loss =0.00560778 For validation epoch 16, loss =0.08828782\n",
      "For training epoch 17, loss =0.09493293 For validation epoch 17, loss =0.00115653\n",
      "For training epoch 18, loss =0.00134821 For validation epoch 18, loss =0.03454955\n",
      "For training epoch 19, loss =0.04904110 For validation epoch 19, loss =0.01649559\n",
      "For training epoch 20, loss =0.00660855 For validation epoch 20, loss =0.00271619\n",
      "For training epoch 21, loss =0.18662965 For validation epoch 21, loss =0.00207427\n",
      "For training epoch 22, loss =0.01381412 For validation epoch 22, loss =0.01362761\n",
      "For training epoch 23, loss =0.01176121 For validation epoch 23, loss =0.02368114\n",
      "For training epoch 24, loss =0.00099754 For validation epoch 24, loss =0.01732405\n",
      "For training epoch 25, loss =0.01467974 For validation epoch 25, loss =0.00248852\n",
      "For training epoch 26, loss =0.00436994 For validation epoch 26, loss =0.02912521\n",
      "For training epoch 27, loss =0.04951442 For validation epoch 27, loss =0.00229942\n",
      "For training epoch 28, loss =0.01281338 For validation epoch 28, loss =0.00800330\n",
      "For training epoch 29, loss =0.02511580 For validation epoch 29, loss =0.00700729\n",
      "For training epoch 30, loss =0.00279743 For validation epoch 30, loss =0.00140444\n",
      "For training epoch 31, loss =0.00630840 For validation epoch 31, loss =0.00114505\n",
      "For training epoch 32, loss =0.00067944 For validation epoch 32, loss =0.00890249\n",
      "For training epoch 33, loss =0.09193923 For validation epoch 33, loss =0.00261784\n",
      "For training epoch 34, loss =0.00624555 For validation epoch 34, loss =0.00210709\n",
      "For training epoch 35, loss =0.07483153 For validation epoch 35, loss =0.12360498\n",
      "For training epoch 36, loss =0.00321985 For validation epoch 36, loss =0.00776302\n",
      "For training epoch 37, loss =0.00104499 For validation epoch 37, loss =0.00172346\n",
      "For training epoch 38, loss =0.00139869 For validation epoch 38, loss =0.00122514\n",
      "For training epoch 39, loss =0.00090287 For validation epoch 39, loss =0.06702605\n",
      "For training epoch 40, loss =0.00607160 For validation epoch 40, loss =0.21298949\n",
      "For training epoch 41, loss =0.00672776 For validation epoch 41, loss =0.00345808\n",
      "For training epoch 42, loss =0.01882418 For validation epoch 42, loss =0.03037519\n",
      "For training epoch 43, loss =0.03450645 For validation epoch 43, loss =0.00060769\n",
      "For training epoch 44, loss =0.00133796 For validation epoch 44, loss =0.01647593\n",
      "For training epoch 45, loss =0.01023691 For validation epoch 45, loss =0.03111477\n",
      "For training epoch 46, loss =0.00335397 For validation epoch 46, loss =0.16878003\n",
      "For training epoch 47, loss =0.01347197 For validation epoch 47, loss =0.00367740\n",
      "For training epoch 48, loss =0.00650012 For validation epoch 48, loss =0.00009755\n",
      "For training epoch 49, loss =0.00211046 For validation epoch 49, loss =0.01168951\n",
      "For training epoch 50, loss =0.00417943 For validation epoch 50, loss =0.00940156\n",
      "For training epoch 51, loss =0.15602942 For validation epoch 51, loss =0.00172835\n",
      "For training epoch 52, loss =0.00932891 For validation epoch 52, loss =0.08499292\n",
      "For training epoch 53, loss =0.01618143 For validation epoch 53, loss =0.00736212\n",
      "For training epoch 54, loss =0.20960221 For validation epoch 54, loss =0.00332530\n",
      "For training epoch 55, loss =0.06792903 For validation epoch 55, loss =0.01218615\n",
      "For training epoch 56, loss =0.01560088 For validation epoch 56, loss =0.01092627\n",
      "For training epoch 57, loss =0.02881474 For validation epoch 57, loss =0.00149441\n",
      "For training epoch 58, loss =0.00077800 For validation epoch 58, loss =0.00277564\n",
      "For training epoch 59, loss =0.03831156 For validation epoch 59, loss =0.00281247\n",
      "For training epoch 60, loss =0.00030704 For validation epoch 60, loss =0.00104645\n",
      "For training epoch 61, loss =0.01137471 For validation epoch 61, loss =0.18165547\n",
      "For training epoch 62, loss =0.00133619 For validation epoch 62, loss =0.00173797\n",
      "For training epoch 63, loss =0.01503552 For validation epoch 63, loss =0.08154717\n",
      "For training epoch 64, loss =0.00286416 For validation epoch 64, loss =0.00156095\n",
      "For training epoch 65, loss =0.00672700 For validation epoch 65, loss =0.00077167\n",
      "For training epoch 66, loss =0.00795281 For validation epoch 66, loss =0.00242212\n",
      "For training epoch 67, loss =0.00840455 For validation epoch 67, loss =0.01715293\n",
      "For training epoch 68, loss =0.00495148 For validation epoch 68, loss =0.00014215\n",
      "For training epoch 69, loss =0.00680375 For validation epoch 69, loss =0.03676333\n",
      "For training epoch 70, loss =0.01289344 For validation epoch 70, loss =0.00702251\n",
      "For training epoch 71, loss =0.00361346 For validation epoch 71, loss =0.01083960\n",
      "For training epoch 72, loss =0.25811607 For validation epoch 72, loss =0.03586905\n",
      "For training epoch 73, loss =0.08093572 For validation epoch 73, loss =0.03366474\n",
      "For training epoch 74, loss =0.17729893 For validation epoch 74, loss =0.00261557\n",
      "For training epoch 75, loss =0.00172208 For validation epoch 75, loss =0.00515160\n",
      "For training epoch 76, loss =0.00249063 For validation epoch 76, loss =0.00749043\n",
      "For training epoch 77, loss =0.00416490 For validation epoch 77, loss =0.00832624\n",
      "For training epoch 78, loss =0.00504273 For validation epoch 78, loss =0.00023619\n",
      "For training epoch 79, loss =0.08887821 For validation epoch 79, loss =0.01307742\n",
      "For training epoch 80, loss =0.12810038 For validation epoch 80, loss =0.00385795\n",
      "For training epoch 81, loss =0.00648171 For validation epoch 81, loss =0.00096373\n",
      "For training epoch 82, loss =0.00913516 For validation epoch 82, loss =0.00036890\n",
      "For training epoch 83, loss =0.00814034 For validation epoch 83, loss =0.13830242\n",
      "For training epoch 84, loss =0.00390526 For validation epoch 84, loss =0.00221065\n",
      "For training epoch 85, loss =0.01168690 For validation epoch 85, loss =0.00076264\n",
      "For training epoch 86, loss =0.00057767 For validation epoch 86, loss =0.02135941\n",
      "For training epoch 87, loss =0.04007815 For validation epoch 87, loss =0.00155678\n",
      "For training epoch 88, loss =0.10479697 For validation epoch 88, loss =0.00174371\n",
      "For training epoch 89, loss =0.00148961 For validation epoch 89, loss =0.00267268\n",
      "For training epoch 90, loss =0.00281334 For validation epoch 90, loss =0.00000143\n",
      "For training epoch 91, loss =0.14219505 For validation epoch 91, loss =0.02299145\n",
      "For training epoch 92, loss =0.00110368 For validation epoch 92, loss =0.00088124\n",
      "For training epoch 93, loss =0.00044829 For validation epoch 93, loss =0.17957442\n",
      "For training epoch 94, loss =0.01310877 For validation epoch 94, loss =0.24259366\n",
      "For training epoch 95, loss =0.00898363 For validation epoch 95, loss =0.00087466\n",
      "For training epoch 96, loss =0.02466562 For validation epoch 96, loss =0.00926948\n",
      "For training epoch 97, loss =0.00046611 For validation epoch 97, loss =0.00525961\n",
      "For training epoch 98, loss =0.00066410 For validation epoch 98, loss =0.00084568\n",
      "For training epoch 99, loss =0.01285946 For validation epoch 99, loss =0.13705923\n",
      "For training epoch 100, loss =0.11228821 For validation epoch 100, loss =0.00024213\n"
     ]
    }
   ],
   "source": [
    "#Train with batch training\n",
    "model = NeuralNet(inputsize, hiddensize1, hiddensize2, outputsize)\n",
    "\n",
    "#Construct the loss and Optimizer\n",
    "learning_rate = 0.001\n",
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_iters = 100\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "t_acc_t = []\n",
    "v_acc_t = []\n",
    "\n",
    "\n",
    "#Train the model\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #Train per batch\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    model.train()\n",
    "    for (x, y) in (xelaloader):\n",
    "      \n",
    "        #empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        #compute the loss\n",
    "        l = loss(y_pred, y)\n",
    "\n",
    "        #compute the gradient\n",
    "        l.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #append each loss per batch\n",
    "        train_loss.append(l.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "        \n",
    "    \n",
    "    t_acc = correct/l_xelaloader\n",
    "    t_acc_t.append(t_acc)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    for (x,y) in (xelaloadervalid):\n",
    "        y_pred_test = model(x)\n",
    "        lv = loss(y_pred_test, y)\n",
    "\n",
    "        #append each loss per batch\n",
    "        valid_loss.append(lv.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred_test.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "    \n",
    "    v_acc = correct/l_xelaloader\n",
    "    v_acc_t.append(v_acc)\n",
    "\n",
    "    #append the total loss and accuracy per epoch\n",
    "    t_loss.append(np.mean(train_loss))\n",
    "    v_loss.append(np.mean(valid_loss))\n",
    "\n",
    "\n",
    "    print(f'For training epoch {epoch+1}, loss ={l:.8f}', f'For validation epoch {epoch+1}, loss ={lv:.8f}'  )\n",
    "\n",
    "torch.save(model.state_dict(), \"DNN.pth\",_use_new_zipfile_serialization=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the loss per epoch for training and validation\n",
    "\n",
    "#create a subplot of 3 plots\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "#set labels\n",
    "ax[0].set_xlabel('Epochs', color = 'black')\n",
    "ax[0].set_ylabel('Loss', color = 'black')\n",
    "ax[1].set_xlabel('Epochs', color = 'black')\n",
    "ax[1].set_ylabel('Accuracy', color = 'black')\n",
    "\n",
    "\n",
    "ax[0].plot(t_loss, label='Train Loss', color='red', linewidth=2, linestyle='dashed')\n",
    "ax[0].plot(v_loss, label='Validation Loss', color = 'blue')\n",
    "ax[1].plot(t_acc_t, label = 'Train Accuracy', color = 'orange')\n",
    "ax[1].plot(v_acc_t, label = 'Validation Accuracy', color = 'green')\n",
    "\n",
    "#set legends\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics after Batch Training\n",
    "print('Metrics after Batch Training')\n",
    "detection_metrics(xela_valid, sliplabel_valid, model_type = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIP PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the list of xela_allfiles and sliplabel_allfiles across axis = 0\n",
    "n = 20 #-> Define the number of time steps for prediction\n",
    "list_xela_allfiles, list_sliplabel_allfiles = read_file(1, n)\n",
    "pd_xela_allfiles = pd.concat(list_xela_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd.concat(list_sliplabel_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd_sliplabel_allfiles['slip']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with imbalanced data using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "# Fit the model to generate the data.\n",
    "pd_xela_allfiles, pd_sliplabel_allfiles = sm.fit_resample(pd_xela_allfiles, pd_sliplabel_allfiles)\n",
    "oversampled = pd.concat([pd.DataFrame(pd_xela_allfiles), pd.DataFrame(pd_sliplabel_allfiles)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy values\n",
    "pd_xela_allfiles = np.array(pd_xela_allfiles.values)\n",
    "pd_sliplabel_allfiles = np.array(pd_sliplabel_allfiles.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and validation\n",
    "xela_train, xela_test, sliplabel_train, sliplabel_test = train_test_split(pd_xela_allfiles, pd_sliplabel_allfiles, shuffle=True, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into validation and holdout\n",
    "xela_train, xela_valid, sliplabel_train, sliplabel_valid = train_test_split(xela_train, sliplabel_train, shuffle = True, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "sc = StandardScaler()\n",
    "xela_train = sc.fit_transform(xela_train)\n",
    "xela_test = sc.transform(xela_test)\n",
    "xela_valid = sc.transform(xela_valid)\n",
    "\n",
    "#convert to tensor values\n",
    "xela_train = torch.from_numpy(xela_train.astype(np.float32))\n",
    "xela_test = torch.from_numpy(xela_test.astype(np.float32))\n",
    "\n",
    "sliplabel_train = torch.from_numpy(sliplabel_train.astype(np.float32))\n",
    "sliplabel_test = torch.from_numpy(sliplabel_test.astype(np.float32))\n",
    "\n",
    "xela_valid = torch.from_numpy(xela_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "sliplabel_valid = torch.from_numpy(sliplabel_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "#reshape the y tensor\n",
    "sliplabel_train = sliplabel_train.view(sliplabel_train.shape[0], 1)\n",
    "sliplabel_test = sliplabel_test.view(sliplabel_test.shape[0], 1)\n",
    "\n",
    "sliplabel_valid = sliplabel_valid.view(sliplabel_valid.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply batch division on the training/validation data\n",
    "class Batchdata(Dataset):\n",
    "    def __init__(self, xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = None):\n",
    "        self.x = xela_train\n",
    "        self.y = sliplabel_train\n",
    "        self.xvalid = xela_valid\n",
    "        self.yvalid = sliplabel_valid\n",
    "        self.valid = valid\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.valid == True:\n",
    "            return self.xvalid.shape[0]\n",
    "        else:\n",
    "            return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.valid == True:\n",
    "            return self.xvalid[idx], self.yvalid[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "#DetAILS FOR BATCH Training\n",
    "dataset = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid)\n",
    "dataset2 = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = True)\n",
    "\n",
    "xelaloader = DataLoader(dataset = dataset, batch_size=32, shuffle=True)\n",
    "xelaloadervalid = DataLoader(dataset = dataset2, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model parameters\n",
    "inputsize = xela_train.shape[1]\n",
    "outputsize = 1\n",
    "hiddensize1 = 16\n",
    "hiddensize2 = 10\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize1, hiddensize2, outputsize):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(inputsize, hiddensize1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hiddensize1, hiddensize2)\n",
    "        self.l3 = nn.Linear(hiddensize2, outputsize)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with batch training\n",
    "model = NeuralNet(inputsize, hiddensize1, hiddensize2, outputsize)\n",
    "\n",
    "#Construct the loss and Optimizer\n",
    "learning_rate = 0.01\n",
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_iters = 100\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "t_acc_t = []\n",
    "v_acc_t = []\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "l_xelaloader = 0\n",
    "\n",
    "#Train the model\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #Train per batch\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "    \n",
    "    model.train()\n",
    "    for (x, y) in (xelaloader):\n",
    "      \n",
    "        #empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        #compute the loss\n",
    "        l = loss(y_pred, y)\n",
    "\n",
    "        #compute the gradient\n",
    "        l.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #append each loss per batch\n",
    "        train_loss.append(l.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "     \n",
    "    \n",
    "    t_acc = correct/l_xelaloader\n",
    "    t_acc_t.append(t_acc)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    for (x,y) in (xelaloadervalid):\n",
    "        y_pred_test = model(x)\n",
    "        lv = loss(y_pred_test, y)\n",
    "\n",
    "        #append each loss per batch\n",
    "        valid_loss.append(lv.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred_test.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "    \n",
    "    v_acc = correct/l_xelaloader\n",
    "    v_acc_t.append(v_acc)\n",
    "\n",
    "    #append the total loss and accuracy per epoch\n",
    "    t_loss.append(np.mean(train_loss))\n",
    "    \n",
    "\n",
    "    v_loss.append(np.mean(valid_loss))\n",
    "\n",
    "\n",
    "    print(f'For training epoch {epoch+1}, loss ={l:.8f}', f'For validation epoch {epoch+1}, loss ={lv:.8f}'  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the loss per epoch for training and validation\n",
    "\n",
    "#create a subplot of 3 plots\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15, 5))\n",
    "\n",
    "#set labels\n",
    "\n",
    "ax[0].set_xlabel('Epochs', color = 'black')\n",
    "ax[0].set_ylabel('Loss', color = 'black')\n",
    "ax[1].set_xlabel('Epochs', color = 'black')\n",
    "ax[1].set_ylabel('Loss', color = 'black')\n",
    "\n",
    "ax[0].plot(t_loss, label='Train Loss', color='red', linewidth=2, linestyle='dashed')\n",
    "ax[0].plot(v_loss, label='Validation Loss', color = 'blue')\n",
    "ax[1].plot(t_acc_t, label = 'Train Accuracy', color = 'orange')\n",
    "ax[1].plot(v_acc_t, label = 'Validation Accuracy', color = 'green')\n",
    "\n",
    "#set legends\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics after batch training for slip prediction\n",
    "print('Metrics after batch training for slip prediction')\n",
    "slip_metrics(n, xela_test, sliplabel_test, modeltype = model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
