{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, f1_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data \n",
    "#algorithm to read all the files\n",
    "\n",
    "'''\n",
    "for folder in this folder:\n",
    "    read xelasensor1.csv\n",
    "    read sliplabel.csv\n",
    "    concat it in a single dataframe along axis = 0\n",
    "\n",
    "print the dataframe\n",
    "'''\n",
    "import os\n",
    "\n",
    "directory = 'CNN-GradCAM/train2dof'\n",
    "directory2 = 'train2dof'\n",
    "\n",
    "def read_file(detect_or_pred, n = None):\n",
    "\n",
    "    #store all directories in a list\n",
    "    list_xela_allfiles = []\n",
    "    list_sliplabel_allfiles = []\n",
    "\n",
    "    for root, subdirectories, files in os.walk(directory):\n",
    "        for sdirectory in subdirectories:\n",
    "\n",
    "            #subdirectory with absolute path\n",
    "            subdirectory = '{}/{}'.format(root, sdirectory)\n",
    "\n",
    "            #read specific files in the subdirectory\n",
    "            for file in os.listdir(subdirectory):\n",
    "            \n",
    "                if file.endswith(\"sensor1.csv\"):\n",
    "                    df = pd.read_csv('{}/{}'.format(subdirectory, file), index_col=None, header=0)\n",
    "                    \n",
    "                    if detect_or_pred ==0:\n",
    "                        list_xela_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None:\n",
    "                        list_xela_allfiles.append(df[:-n])\n",
    "\n",
    "                if file.endswith(\"label.csv\"):\n",
    "                    df = pd.read_csv('{}/{}'.format(subdirectory, file), index_col=None, header=0)\n",
    "                    if detect_or_pred ==0:\n",
    "                        list_sliplabel_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None: \n",
    "                        list_sliplabel_allfiles.append(df[n:])\n",
    "\n",
    "    return list_xela_allfiles, list_sliplabel_allfiles\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRICS DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detection_metrics(xela_test, sliplabel_test, model_type, acc = None):\n",
    "    #predict using the holdout set (DONE)\n",
    "    predicted = model_type(xela_test).detach().numpy()\n",
    "    predicted_cls = predicted.round()\n",
    "\n",
    "    #Plot the loss values against number of epochs (DONE)\n",
    "    #validation test (DONE)\n",
    "\n",
    "    #Print the accuracy\n",
    "    accuracy = accuracy_score(sliplabel_test.numpy(), predicted_cls)\n",
    "    print(f'Accuracy for slip detection is {accuracy}')\n",
    "\n",
    "    #Print the fscore\n",
    "    fscore = f1_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Fscore for slip detection is {fscore}')\n",
    "\n",
    "    #print the Precision\n",
    "    precision = precision_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Precision for slip detection is {precision}')\n",
    "\n",
    "    #print the Recall\n",
    "    recall = recall_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Recall for slip detection is {recall}')\n",
    "\n",
    "def slip_metrics(n, xela_test, sliplabel_test, modeltype):\n",
    "    #predict using the holdout set (DONE)\n",
    "    predicted = modeltype(xela_test).detach().numpy()\n",
    "    predicted_cls = predicted.round()\n",
    "\n",
    "    #Plot the loss values against number of epochs (DONE)\n",
    "    #validation test (DONE)\n",
    "\n",
    "    #Print the accuracy\n",
    "    x = 0\n",
    "    for i in range(predicted_cls.shape[0]):\n",
    "        if predicted_cls[i].item() == sliplabel_test[i].item():\n",
    "            x += 1\n",
    "\n",
    "    accuracy = x/ float(sliplabel_test.shape[0])\n",
    "    print(f'Accuracy for slip prediction for (t+{n}) is {accuracy}')\n",
    "\n",
    "    #Print the fscore\n",
    "    fscore = f1_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Fscore for slip prediction for (t+{n}) is {fscore}')\n",
    "\n",
    "    #print the Precision\n",
    "    precision = precision_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Precision for slip prediction for (t+{n}) is {precision}')\n",
    "\n",
    "    #print the Recall\n",
    "    recall = recall_score(sliplabel_test.numpy(), predicted_cls, average='macro')\n",
    "    print(f'Recall for slip prediction for (t+{n}) is {recall}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIP DETECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the list of xela_allfiles and sliplabel_allfiles across axis = 0\n",
    "list_xela_allfiles, list_sliplabel_allfiles = read_file(0)\n",
    "pd_xela_allfiles = pd.concat(list_xela_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd.concat(list_sliplabel_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd_sliplabel_allfiles['slip']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dealing with imbalanced data using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "# Fit the model to generate the data.\n",
    "pd_xela_allfiles, pd_sliplabel_allfiles = sm.fit_resample(pd_xela_allfiles, pd_sliplabel_allfiles)\n",
    "oversampled = pd.concat([pd.DataFrame(pd_xela_allfiles), pd.DataFrame(pd_sliplabel_allfiles)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy values\n",
    "pd_xela_allfiles = np.array(pd_xela_allfiles.values)\n",
    "pd_sliplabel_allfiles = np.array(pd_sliplabel_allfiles.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test\n",
    "xela_train, xela_test, sliplabel_train, sliplabel_test = train_test_split(pd_xela_allfiles, pd_sliplabel_allfiles, shuffle=True, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into validation and holdout\n",
    "xela_train, xela_valid, sliplabel_train, sliplabel_valid = train_test_split(xela_train, sliplabel_train, shuffle = True, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([114802, 48])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xela_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "sc = StandardScaler()\n",
    "xela_train = sc.fit_transform(xela_train)\n",
    "xela_test = sc.transform(xela_test)\n",
    "xela_valid = sc.transform(xela_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tensor values\n",
    "xela_train = torch.from_numpy(xela_train.astype(np.float32))\n",
    "xela_test = torch.from_numpy(xela_test.astype(np.float32))\n",
    "\n",
    "sliplabel_train = torch.from_numpy(sliplabel_train.astype(np.float32))\n",
    "sliplabel_test = torch.from_numpy(sliplabel_test.astype(np.float32))\n",
    "\n",
    "xela_valid = torch.from_numpy(xela_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "sliplabel_valid = torch.from_numpy(sliplabel_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "#reshape the y tensor\n",
    "sliplabel_train = sliplabel_train.view(sliplabel_train.shape[0], 1)\n",
    "sliplabel_test = sliplabel_test.view(sliplabel_test.shape[0], 1)\n",
    "\n",
    "sliplabel_valid = sliplabel_valid.view(sliplabel_valid.shape[0], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply batch training on the training/validation data\n",
    "class Batchdata(Dataset):\n",
    "    def __init__(self, xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = None):\n",
    "        self.x = xela_train\n",
    "        self.y = sliplabel_train\n",
    "        self.xvalid = xela_valid\n",
    "        self.yvalid = sliplabel_valid\n",
    "        self.valid = valid\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.valid == True:\n",
    "            return self.xvalid.shape[0]\n",
    "        else:\n",
    "            return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.valid == True:\n",
    "            return self.xvalid[idx], self.yvalid[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "dataset = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid)\n",
    "dataset2 = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = True)\n",
    "\n",
    "xelaloader = DataLoader(dataset = dataset, batch_size=32, shuffle=True)\n",
    "xelaloadervalid = DataLoader(dataset = dataset2, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the NN class\n",
    "\n",
    "inputsize = xela_train.shape[1]\n",
    "outputsize = 1\n",
    "hiddensize1 = 16\n",
    "hiddensize2 = 10\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize1, hiddensize2, outputsize):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(inputsize, hiddensize1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hiddensize1, hiddensize2)\n",
    "        self.l3 = nn.Linear(hiddensize2, outputsize)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet(inputsize, hiddensize1, hiddensize2, outputsize).to('cpu')\n",
    "model.load_state_dict(torch.load('models/DNN.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = sliplabel_valid\n",
    "model.eval()  # set model to evaluation mode\n",
    "with torch.no_grad():  # disable gradient computation\n",
    "    y_pred = model(xela_valid)\n",
    "    y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "class_names = [\"No-slip\", \"Slip\"]  # 0 → No-slip, 1 → Slip\n",
    "\n",
    "# 3. Display with labels and title\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "# 4. Plot \n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "disp.plot(cmap='Blues', ax=ax)\n",
    "plt.title(\"Confusion Matrix SMOTE-NN Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training epoch 1, loss =0.12242735 For validation epoch 1, loss =0.11427331\n",
      "For training epoch 2, loss =0.07393983 For validation epoch 2, loss =0.01158634\n",
      "For training epoch 3, loss =0.13840209 For validation epoch 3, loss =0.02297569\n",
      "For training epoch 4, loss =0.09802894 For validation epoch 4, loss =0.14555718\n",
      "For training epoch 5, loss =0.00711670 For validation epoch 5, loss =0.01778943\n",
      "For training epoch 6, loss =0.00263717 For validation epoch 6, loss =0.02079939\n",
      "For training epoch 7, loss =0.01130870 For validation epoch 7, loss =0.00609502\n",
      "For training epoch 8, loss =0.01599028 For validation epoch 8, loss =0.08565203\n",
      "For training epoch 9, loss =0.22539422 For validation epoch 9, loss =0.03270976\n",
      "For training epoch 10, loss =0.06984029 For validation epoch 10, loss =0.03342824\n",
      "For training epoch 11, loss =0.00904876 For validation epoch 11, loss =0.00109306\n",
      "For training epoch 12, loss =0.02612330 For validation epoch 12, loss =0.00412355\n",
      "For training epoch 13, loss =0.00196989 For validation epoch 13, loss =0.00542692\n",
      "For training epoch 14, loss =0.08675880 For validation epoch 14, loss =0.12457057\n",
      "For training epoch 15, loss =0.00233277 For validation epoch 15, loss =0.00287283\n",
      "For training epoch 16, loss =0.00560778 For validation epoch 16, loss =0.08828782\n",
      "For training epoch 17, loss =0.09493293 For validation epoch 17, loss =0.00115653\n",
      "For training epoch 18, loss =0.00134821 For validation epoch 18, loss =0.03454955\n",
      "For training epoch 19, loss =0.04904110 For validation epoch 19, loss =0.01649559\n",
      "For training epoch 20, loss =0.00660855 For validation epoch 20, loss =0.00271619\n",
      "For training epoch 21, loss =0.18662965 For validation epoch 21, loss =0.00207427\n",
      "For training epoch 22, loss =0.01381412 For validation epoch 22, loss =0.01362761\n",
      "For training epoch 23, loss =0.01176121 For validation epoch 23, loss =0.02368114\n",
      "For training epoch 24, loss =0.00099754 For validation epoch 24, loss =0.01732405\n",
      "For training epoch 25, loss =0.01467974 For validation epoch 25, loss =0.00248852\n",
      "For training epoch 26, loss =0.00436994 For validation epoch 26, loss =0.02912521\n",
      "For training epoch 27, loss =0.04951442 For validation epoch 27, loss =0.00229942\n",
      "For training epoch 28, loss =0.01281338 For validation epoch 28, loss =0.00800330\n",
      "For training epoch 29, loss =0.02511580 For validation epoch 29, loss =0.00700729\n",
      "For training epoch 30, loss =0.00279743 For validation epoch 30, loss =0.00140444\n",
      "For training epoch 31, loss =0.00630840 For validation epoch 31, loss =0.00114505\n",
      "For training epoch 32, loss =0.00067944 For validation epoch 32, loss =0.00890249\n",
      "For training epoch 33, loss =0.09193923 For validation epoch 33, loss =0.00261784\n",
      "For training epoch 34, loss =0.00624555 For validation epoch 34, loss =0.00210709\n",
      "For training epoch 35, loss =0.07483153 For validation epoch 35, loss =0.12360498\n",
      "For training epoch 36, loss =0.00321985 For validation epoch 36, loss =0.00776302\n",
      "For training epoch 37, loss =0.00104499 For validation epoch 37, loss =0.00172346\n",
      "For training epoch 38, loss =0.00139869 For validation epoch 38, loss =0.00122514\n",
      "For training epoch 39, loss =0.00090287 For validation epoch 39, loss =0.06702605\n",
      "For training epoch 40, loss =0.00607160 For validation epoch 40, loss =0.21298949\n",
      "For training epoch 41, loss =0.00672776 For validation epoch 41, loss =0.00345808\n",
      "For training epoch 42, loss =0.01882418 For validation epoch 42, loss =0.03037519\n",
      "For training epoch 43, loss =0.03450645 For validation epoch 43, loss =0.00060769\n",
      "For training epoch 44, loss =0.00133796 For validation epoch 44, loss =0.01647593\n",
      "For training epoch 45, loss =0.01023691 For validation epoch 45, loss =0.03111477\n",
      "For training epoch 46, loss =0.00335397 For validation epoch 46, loss =0.16878003\n",
      "For training epoch 47, loss =0.01347197 For validation epoch 47, loss =0.00367740\n",
      "For training epoch 48, loss =0.00650012 For validation epoch 48, loss =0.00009755\n",
      "For training epoch 49, loss =0.00211046 For validation epoch 49, loss =0.01168951\n",
      "For training epoch 50, loss =0.00417943 For validation epoch 50, loss =0.00940156\n",
      "For training epoch 51, loss =0.15602942 For validation epoch 51, loss =0.00172835\n",
      "For training epoch 52, loss =0.00932891 For validation epoch 52, loss =0.08499292\n",
      "For training epoch 53, loss =0.01618143 For validation epoch 53, loss =0.00736212\n",
      "For training epoch 54, loss =0.20960221 For validation epoch 54, loss =0.00332530\n",
      "For training epoch 55, loss =0.06792903 For validation epoch 55, loss =0.01218615\n",
      "For training epoch 56, loss =0.01560088 For validation epoch 56, loss =0.01092627\n",
      "For training epoch 57, loss =0.02881474 For validation epoch 57, loss =0.00149441\n",
      "For training epoch 58, loss =0.00077800 For validation epoch 58, loss =0.00277564\n",
      "For training epoch 59, loss =0.03831156 For validation epoch 59, loss =0.00281247\n",
      "For training epoch 60, loss =0.00030704 For validation epoch 60, loss =0.00104645\n",
      "For training epoch 61, loss =0.01137471 For validation epoch 61, loss =0.18165547\n",
      "For training epoch 62, loss =0.00133619 For validation epoch 62, loss =0.00173797\n",
      "For training epoch 63, loss =0.01503552 For validation epoch 63, loss =0.08154717\n",
      "For training epoch 64, loss =0.00286416 For validation epoch 64, loss =0.00156095\n",
      "For training epoch 65, loss =0.00672700 For validation epoch 65, loss =0.00077167\n",
      "For training epoch 66, loss =0.00795281 For validation epoch 66, loss =0.00242212\n",
      "For training epoch 67, loss =0.00840455 For validation epoch 67, loss =0.01715293\n",
      "For training epoch 68, loss =0.00495148 For validation epoch 68, loss =0.00014215\n",
      "For training epoch 69, loss =0.00680375 For validation epoch 69, loss =0.03676333\n",
      "For training epoch 70, loss =0.01289344 For validation epoch 70, loss =0.00702251\n",
      "For training epoch 71, loss =0.00361346 For validation epoch 71, loss =0.01083960\n",
      "For training epoch 72, loss =0.25811607 For validation epoch 72, loss =0.03586905\n",
      "For training epoch 73, loss =0.08093572 For validation epoch 73, loss =0.03366474\n",
      "For training epoch 74, loss =0.17729893 For validation epoch 74, loss =0.00261557\n",
      "For training epoch 75, loss =0.00172208 For validation epoch 75, loss =0.00515160\n",
      "For training epoch 76, loss =0.00249063 For validation epoch 76, loss =0.00749043\n",
      "For training epoch 77, loss =0.00416490 For validation epoch 77, loss =0.00832624\n",
      "For training epoch 78, loss =0.00504273 For validation epoch 78, loss =0.00023619\n",
      "For training epoch 79, loss =0.08887821 For validation epoch 79, loss =0.01307742\n",
      "For training epoch 80, loss =0.12810038 For validation epoch 80, loss =0.00385795\n",
      "For training epoch 81, loss =0.00648171 For validation epoch 81, loss =0.00096373\n",
      "For training epoch 82, loss =0.00913516 For validation epoch 82, loss =0.00036890\n",
      "For training epoch 83, loss =0.00814034 For validation epoch 83, loss =0.13830242\n",
      "For training epoch 84, loss =0.00390526 For validation epoch 84, loss =0.00221065\n",
      "For training epoch 85, loss =0.01168690 For validation epoch 85, loss =0.00076264\n",
      "For training epoch 86, loss =0.00057767 For validation epoch 86, loss =0.02135941\n",
      "For training epoch 87, loss =0.04007815 For validation epoch 87, loss =0.00155678\n",
      "For training epoch 88, loss =0.10479697 For validation epoch 88, loss =0.00174371\n",
      "For training epoch 89, loss =0.00148961 For validation epoch 89, loss =0.00267268\n",
      "For training epoch 90, loss =0.00281334 For validation epoch 90, loss =0.00000143\n",
      "For training epoch 91, loss =0.14219505 For validation epoch 91, loss =0.02299145\n",
      "For training epoch 92, loss =0.00110368 For validation epoch 92, loss =0.00088124\n",
      "For training epoch 93, loss =0.00044829 For validation epoch 93, loss =0.17957442\n",
      "For training epoch 94, loss =0.01310877 For validation epoch 94, loss =0.24259366\n",
      "For training epoch 95, loss =0.00898363 For validation epoch 95, loss =0.00087466\n",
      "For training epoch 96, loss =0.02466562 For validation epoch 96, loss =0.00926948\n",
      "For training epoch 97, loss =0.00046611 For validation epoch 97, loss =0.00525961\n",
      "For training epoch 98, loss =0.00066410 For validation epoch 98, loss =0.00084568\n",
      "For training epoch 99, loss =0.01285946 For validation epoch 99, loss =0.13705923\n",
      "For training epoch 100, loss =0.11228821 For validation epoch 100, loss =0.00024213\n"
     ]
    }
   ],
   "source": [
    "#Train with batch training\n",
    "model = NeuralNet(inputsize, hiddensize1, hiddensize2, outputsize)\n",
    "\n",
    "#Construct the loss and Optimizer\n",
    "learning_rate = 0.001\n",
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_iters = 100\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "t_acc_t = []\n",
    "v_acc_t = []\n",
    "\n",
    "\n",
    "#Train the model\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #Train per batch\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    model.train()\n",
    "    for (x, y) in (xelaloader):\n",
    "      \n",
    "        #empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        #compute the loss\n",
    "        l = loss(y_pred, y)\n",
    "\n",
    "        #compute the gradient\n",
    "        l.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #append each loss per batch\n",
    "        train_loss.append(l.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "        \n",
    "    \n",
    "    t_acc = correct/l_xelaloader\n",
    "    t_acc_t.append(t_acc)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    for (x,y) in (xelaloadervalid):\n",
    "        y_pred_test = model(x)\n",
    "        lv = loss(y_pred_test, y)\n",
    "\n",
    "        #append each loss per batch\n",
    "        valid_loss.append(lv.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred_test.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "    \n",
    "    v_acc = correct/l_xelaloader\n",
    "    v_acc_t.append(v_acc)\n",
    "\n",
    "    #append the total loss and accuracy per epoch\n",
    "    t_loss.append(np.mean(train_loss))\n",
    "    v_loss.append(np.mean(valid_loss))\n",
    "\n",
    "\n",
    "    print(f'For training epoch {epoch+1}, loss ={l:.8f}', f'For validation epoch {epoch+1}, loss ={lv:.8f}'  )\n",
    "\n",
    "torch.save(model.state_dict(), \"DNN.pth\",_use_new_zipfile_serialization=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the loss per epoch for training and validation\n",
    "\n",
    "#create a subplot of 3 plots\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "#set labels\n",
    "ax[0].set_xlabel('Epochs', color = 'black')\n",
    "ax[0].set_ylabel('Loss', color = 'black')\n",
    "ax[1].set_xlabel('Epochs', color = 'black')\n",
    "ax[1].set_ylabel('Accuracy', color = 'black')\n",
    "\n",
    "\n",
    "ax[0].plot(t_loss, label='Train Loss', color='red', linewidth=2, linestyle='dashed')\n",
    "ax[0].plot(v_loss, label='Validation Loss', color = 'blue')\n",
    "ax[1].plot(t_acc_t, label = 'Train Accuracy', color = 'orange')\n",
    "ax[1].plot(v_acc_t, label = 'Validation Accuracy', color = 'green')\n",
    "\n",
    "#set legends\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics after Batch Training\n",
    "print('Metrics after Batch Training')\n",
    "detection_metrics(xela_valid, sliplabel_valid, model_type = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIP PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the list of xela_allfiles and sliplabel_allfiles across axis = 0\n",
    "n = 20 #-> Define the number of time steps for prediction\n",
    "list_xela_allfiles, list_sliplabel_allfiles = read_file(1, n)\n",
    "pd_xela_allfiles = pd.concat(list_xela_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd.concat(list_sliplabel_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd_sliplabel_allfiles['slip']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with imbalanced data using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "# Fit the model to generate the data.\n",
    "pd_xela_allfiles, pd_sliplabel_allfiles = sm.fit_resample(pd_xela_allfiles, pd_sliplabel_allfiles)\n",
    "oversampled = pd.concat([pd.DataFrame(pd_xela_allfiles), pd.DataFrame(pd_sliplabel_allfiles)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy values\n",
    "pd_xela_allfiles = np.array(pd_xela_allfiles.values)\n",
    "pd_sliplabel_allfiles = np.array(pd_sliplabel_allfiles.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and validation\n",
    "xela_train, xela_test, sliplabel_train, sliplabel_test = train_test_split(pd_xela_allfiles, pd_sliplabel_allfiles, shuffle=True, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into validation and holdout\n",
    "xela_train, xela_valid, sliplabel_train, sliplabel_valid = train_test_split(xela_train, sliplabel_train, shuffle = True, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "sc = StandardScaler()\n",
    "xela_train = sc.fit_transform(xela_train)\n",
    "xela_test = sc.transform(xela_test)\n",
    "xela_valid = sc.transform(xela_valid)\n",
    "\n",
    "#convert to tensor values\n",
    "xela_train = torch.from_numpy(xela_train.astype(np.float32))\n",
    "xela_test = torch.from_numpy(xela_test.astype(np.float32))\n",
    "\n",
    "sliplabel_train = torch.from_numpy(sliplabel_train.astype(np.float32))\n",
    "sliplabel_test = torch.from_numpy(sliplabel_test.astype(np.float32))\n",
    "\n",
    "xela_valid = torch.from_numpy(xela_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "sliplabel_valid = torch.from_numpy(sliplabel_valid.astype(np.float32))\n",
    "\n",
    "\n",
    "#reshape the y tensor\n",
    "sliplabel_train = sliplabel_train.view(sliplabel_train.shape[0], 1)\n",
    "sliplabel_test = sliplabel_test.view(sliplabel_test.shape[0], 1)\n",
    "\n",
    "sliplabel_valid = sliplabel_valid.view(sliplabel_valid.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply batch division on the training/validation data\n",
    "class Batchdata(Dataset):\n",
    "    def __init__(self, xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = None):\n",
    "        self.x = xela_train\n",
    "        self.y = sliplabel_train\n",
    "        self.xvalid = xela_valid\n",
    "        self.yvalid = sliplabel_valid\n",
    "        self.valid = valid\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.valid == True:\n",
    "            return self.xvalid.shape[0]\n",
    "        else:\n",
    "            return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.valid == True:\n",
    "            return self.xvalid[idx], self.yvalid[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "#DetAILS FOR BATCH Training\n",
    "dataset = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid)\n",
    "dataset2 = Batchdata(xela_train, sliplabel_train, xela_valid, sliplabel_valid, valid = True)\n",
    "\n",
    "xelaloader = DataLoader(dataset = dataset, batch_size=32, shuffle=True)\n",
    "xelaloadervalid = DataLoader(dataset = dataset2, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model parameters\n",
    "inputsize = xela_train.shape[1]\n",
    "outputsize = 1\n",
    "hiddensize1 = 16\n",
    "hiddensize2 = 10\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize1, hiddensize2, outputsize):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(inputsize, hiddensize1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hiddensize1, hiddensize2)\n",
    "        self.l3 = nn.Linear(hiddensize2, outputsize)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with batch training\n",
    "model = NeuralNet(inputsize, hiddensize1, hiddensize2, outputsize)\n",
    "\n",
    "#Construct the loss and Optimizer\n",
    "learning_rate = 0.01\n",
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_iters = 100\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "t_acc_t = []\n",
    "v_acc_t = []\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "l_xelaloader = 0\n",
    "\n",
    "#Train the model\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #Train per batch\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "    \n",
    "    model.train()\n",
    "    for (x, y) in (xelaloader):\n",
    "      \n",
    "        #empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        #compute the loss\n",
    "        l = loss(y_pred, y)\n",
    "\n",
    "        #compute the gradient\n",
    "        l.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #append each loss per batch\n",
    "        train_loss.append(l.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "     \n",
    "    \n",
    "    t_acc = correct/l_xelaloader\n",
    "    t_acc_t.append(t_acc)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    l_xelaloader = 0\n",
    "\n",
    "    #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    for (x,y) in (xelaloadervalid):\n",
    "        y_pred_test = model(x)\n",
    "        lv = loss(y_pred_test, y)\n",
    "\n",
    "        #append each loss per batch\n",
    "        valid_loss.append(lv.item())\n",
    "\n",
    "        #accuracy\n",
    "        total += y.size(0)\n",
    "        correct += y_pred_test.round().eq(y).sum().item()\n",
    "        l_xelaloader += x.shape[0]\n",
    "    \n",
    "    v_acc = correct/l_xelaloader\n",
    "    v_acc_t.append(v_acc)\n",
    "\n",
    "    #append the total loss and accuracy per epoch\n",
    "    t_loss.append(np.mean(train_loss))\n",
    "    \n",
    "\n",
    "    v_loss.append(np.mean(valid_loss))\n",
    "\n",
    "\n",
    "    print(f'For training epoch {epoch+1}, loss ={l:.8f}', f'For validation epoch {epoch+1}, loss ={lv:.8f}'  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the loss per epoch for training and validation\n",
    "\n",
    "#create a subplot of 3 plots\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15, 5))\n",
    "\n",
    "#set labels\n",
    "\n",
    "ax[0].set_xlabel('Epochs', color = 'black')\n",
    "ax[0].set_ylabel('Loss', color = 'black')\n",
    "ax[1].set_xlabel('Epochs', color = 'black')\n",
    "ax[1].set_ylabel('Loss', color = 'black')\n",
    "\n",
    "ax[0].plot(t_loss, label='Train Loss', color='red', linewidth=2, linestyle='dashed')\n",
    "ax[0].plot(v_loss, label='Validation Loss', color = 'blue')\n",
    "ax[1].plot(t_acc_t, label = 'Train Accuracy', color = 'orange')\n",
    "ax[1].plot(v_acc_t, label = 'Validation Accuracy', color = 'green')\n",
    "\n",
    "#set legends\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics after batch training for slip prediction\n",
    "print('Metrics after batch training for slip prediction')\n",
    "slip_metrics(n, xela_test, sliplabel_test, modeltype = model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
