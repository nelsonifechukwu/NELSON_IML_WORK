{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset  # Gives easier dataset managment by creating mini batches etc.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, recall_score, precision_score, accuracy_score, confusion_matrix\n",
    "from ViT_model import VisionTransformer\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.has_mps:\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "directory = '/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/train2dof'\n",
    "\n",
    "def read_file(detect_or_pred, n = None):\n",
    "\n",
    "    #store all directories in a list\n",
    "    list_xela_allfiles = []\n",
    "    list_sliplabel_allfiles = []\n",
    "\n",
    "    for root, subdirectories, files in os.walk(directory):\n",
    "        for sdirectory in subdirectories:\n",
    "\n",
    "            #subdirectory with absolute path\n",
    "            subdirectory = '{}/{}'.format(root, sdirectory)\n",
    "\n",
    "            #read specific files in the subdirectory\n",
    "            for file in os.listdir(subdirectory):\n",
    "            \n",
    "                if not file.startswith(\".\") and file.endswith(\"sensor1.csv\"):\n",
    "                    df = pd.read_csv(f'{subdirectory}/{file}', index_col=None, header=0)\n",
    "                    \n",
    "                    if detect_or_pred ==0:\n",
    "                        list_xela_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None:\n",
    "                        list_xela_allfiles.append(df[:-n])\n",
    "\n",
    "                if not file.startswith(\".\") and file.endswith(\"label.csv\"):\n",
    "                    df = pd.read_csv('{}/{}'.format(subdirectory, file), index_col=None, header=0)\n",
    "                    if detect_or_pred ==0:\n",
    "                        list_sliplabel_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None: \n",
    "                        list_sliplabel_allfiles.append(df[n:])\n",
    "\n",
    "    return list_xela_allfiles, list_sliplabel_allfiles\n",
    "\n",
    "    #np.newaxis; np.zeros (3,4,4) -> \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the list of xela_allfiles and sliplabel_allfiles across axis = 0\n",
    "n = 5\n",
    "# list_xela_allfiles, list_sliplabel_allfiles = read_file(0)\n",
    "\n",
    "#for slip prediction, comment the line above and uncomment the line below\n",
    "list_xela_allfiles, list_sliplabel_allfiles = read_file(1, n)\n",
    "\n",
    "pd_xela_allfiles = pd.concat(list_xela_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd.concat(list_sliplabel_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd_sliplabel_allfiles['slip']\n",
    "\n",
    "#reshape the target array into (rows, 1)\n",
    "tac_label = pd_sliplabel_allfiles.values.reshape(pd_sliplabel_allfiles.shape[0], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['txl1_x', 'txl1_y', 'txl1_z', 'txl2_x', 'txl2_y', 'txl2_z', 'txl3_x',\n",
       "       'txl3_y', 'txl3_z', 'txl4_x', 'txl4_y', 'txl4_z', 'txl5_x', 'txl5_y',\n",
       "       'txl5_z', 'txl6_x', 'txl6_y', 'txl6_z', 'txl7_x', 'txl7_y', 'txl7_z',\n",
       "       'txl8_x', 'txl8_y', 'txl8_z', 'txl9_x', 'txl9_y', 'txl9_z', 'txl10_x',\n",
       "       'txl10_y', 'txl10_z', 'txl11_x', 'txl11_y', 'txl11_z', 'txl12_x',\n",
       "       'txl12_y', 'txl12_z', 'txl13_x', 'txl13_y', 'txl13_z', 'txl14_x',\n",
       "       'txl14_y', 'txl14_z', 'txl15_x', 'txl15_y', 'txl15_z', 'txl16_x',\n",
       "       'txl16_y', 'txl16_z'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd_sliplabel_allfiles.to_csv('labels.csv')\n",
    "pd_xela_allfiles.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RE-ARRANGEMENT OF TABULAR DATA INTO IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrange the data by 3, 4, 4\n",
    "\n",
    "#arrange the columns by x, y, z\n",
    "col_x = []\n",
    "col_y = []\n",
    "col_z = []\n",
    "\n",
    "pd_columns = pd_xela_allfiles.columns\n",
    "for col in pd_columns:\n",
    "    if col.endswith('x'):\n",
    "        col_x.append(col)\n",
    "    \n",
    "    elif col.endswith('y'):\n",
    "        col_y.append(col)\n",
    "    \n",
    "    elif col.endswith('z'):\n",
    "        col_z.append(col)\n",
    "\n",
    "#arrange the table using the arranged columns\n",
    "pd_xela_allfiles_x = pd_xela_allfiles[col_x]\n",
    "pd_xela_allfiles_y = pd_xela_allfiles[col_y]\n",
    "pd_xela_allfiles_z = pd_xela_allfiles[col_z]\n",
    "\n",
    "\n",
    "#scale the data in the arranged columns\n",
    "#scale the data of the features\n",
    "\n",
    "sc = MinMaxScaler() #standard scaler\n",
    "sc.fit(pd_xela_allfiles_x)\n",
    "pd_xela_allfiles_x = sc.transform(pd_xela_allfiles_x)\n",
    "\n",
    "sc.fit(pd_xela_allfiles_y)\n",
    "pd_xela_allfiles_y = sc.transform(pd_xela_allfiles_y)\n",
    "\n",
    "sc.fit(pd_xela_allfiles_z)\n",
    "pd_xela_allfiles_z = sc.transform(pd_xela_allfiles_z)\n",
    "\n",
    "\n",
    "\n",
    "#reshape the arranged data per row to (4,4) AND rotate 90 degree anti-clockwise and append to a list\n",
    "pd_x = []\n",
    "pd_y = []\n",
    "pd_z = []\n",
    "\n",
    "for row in range(len(pd_xela_allfiles_x)):\n",
    "    pd_x.append(np.rot90(pd_xela_allfiles_x[row].reshape(4,4)))\n",
    "    pd_y.append(np.rot90(pd_xela_allfiles_y[row].reshape(4,4)))\n",
    "    pd_z.append(np.rot90(pd_xela_allfiles_z[row].reshape(4,4)))\n",
    "\n",
    "#add all the x, y, z in a single list\n",
    "pd_main = [pd_x, pd_y, pd_z]\n",
    "\n",
    "#arrange pd_main in a 3, 4, 4 array where its 3(4, 4) of x, y, z values\n",
    "pd_image = np.zeros( (pd_xela_allfiles.shape[0], 3, 4, 4))\n",
    "\n",
    "#per row, get (4,4) of x, y, z and assign it to pd_image to form the image\n",
    "for row in range(pd_xela_allfiles.shape[0]):\n",
    "    x_4_4 = pd_main[0][row]\n",
    "    y_4_4 = pd_main[1][row]\n",
    "    z_4_4 = pd_main[2][row]\n",
    "\n",
    "    pd_image[row][0] = x_4_4\n",
    "    pd_image[row][1] = y_4_4\n",
    "    pd_image[row][2] = z_4_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_label = pd_sliplabel_allfiles.values.reshape(pd_sliplabel_allfiles.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226016, 3, 4, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd_image.shape)\n",
    "plt.imshow(pd_image[0][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_image = torch.from_numpy(pd_image.astype(np.float32))\n",
    "pd_label = torch.from_numpy(pd_label.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_image_train, pd_image_test, pd_label_train, pd_label_test = train_test_split(pd_image, pd_label, test_size=0.1, shuffle=True)\n",
    "\n",
    "#split into train and validation\n",
    "pd_image_train, pd_image_valid, pd_label_train, pd_label_valid = train_test_split(pd_image_train, pd_label_train, test_size=0.3, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([142389, 3, 4, 4]), torch.Size([142389, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_image_train.shape, pd_label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq_dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.X.__len__()\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = seq_dataset(pd_image_train, pd_label_train)\n",
    "valid_dataset = seq_dataset(pd_image_valid, pd_label_valid)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4096, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 3, 4, 4]) torch.Size([4096, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    x_try = i[0]\n",
    "    print(i[0].shape, i[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(image_size=4, patch_size=1, in_chans=3, n_classes=1,\n",
    "                  embed_dim=32, depth=2, n_heads=2, mlp_ratio=1., \n",
    "                  qkv_bias=True, p=0.1, attn_p=0.1, proj_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device))\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.5820 , val_loss = 0.4473\n",
      "Epoch 2, loss = 0.4468 , val_loss = 0.4331\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/vit/ViT_New_Train.ipynb Cell 18\u001b[0m line \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/vit/ViT_New_Train.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m valid_accuracy \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/vit/ViT_New_Train.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/vit/ViT_New_Train.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m (x, y) \u001b[39min\u001b[39;00m (train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/vit/ViT_New_Train.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#Forward pass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/vit/ViT_New_Train.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model(x\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/vit/ViT_New_Train.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m#compute the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/nei/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/nei/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/nei/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniforge3/envs/nei/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/vit/ViT_New_Train.ipynb Cell 18\u001b[0m line \u001b[0;36mseq_dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/vit/ViT_New_Train.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/vit/ViT_New_Train.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX[index], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training and validation loop \n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Train per batch\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "\n",
    "    model.train()\n",
    "    for (x, y) in (train_loader):\n",
    "        #Forward pass\n",
    "        y_pred = model(x.to(device))\n",
    "        #compute the loss\n",
    "        l = criterion(y_pred.to(device), y.to(device))\n",
    "        #empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "        #compute the gradient\n",
    "        l.backward()\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "        #append each loss per batch\n",
    "        train_loss.append(l.item())\n",
    "        train_accuracy.append(accuracy_score(y.detach().cpu().numpy().round(), y_pred.detach().cpu().numpy().round()))\n",
    "        \n",
    "    \n",
    "\n",
    "    #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in (valid_loader):\n",
    "            y_pred = model(x.to(device))\n",
    "            lv = criterion(y_pred.to(device), y.to(device))\n",
    "            #append the loss per batch\n",
    "            valid_loss.append(lv.item())\n",
    "            #accuracy\n",
    "            valid_accuracy.append(accuracy_score(y.detach().cpu().numpy().round(), y_pred.detach().cpu().numpy().round()))\n",
    "\n",
    "    #append the total loss and accuracy per epoch\n",
    "    t_loss.append(np.mean(train_loss))\n",
    "    v_loss.append(np.mean(valid_loss))\n",
    "    t_acc.append(np.mean(train_accuracy))\n",
    "    v_acc.append(np.mean(valid_accuracy))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, loss = {np.mean(train_loss):.4f} , val_loss = {np.mean(valid_loss):.4f}')\n",
    "    torch.save(model.state_dict(), 'ViT_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'ViT_n{n}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(metric, title):\n",
    "    plt.plot(metric)\n",
    "    plt.title(title)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(t_loss, \"Train Loss\")\n",
    "plot(v_loss, \"Val Loss\")\n",
    "\n",
    "plot(t_acc, \"Train Accuracy\")\n",
    "plot(v_acc, \"Val Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "947f030b3e678118fc438144c1e47ca5c23949e6feee86165ca58c1240ce2eba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
