{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset  # Gives easier dataset managment by creating mini batches etc.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, recall_score, precision_score, accuracy_score, confusion_matrix\n",
    "from ViT_model import VisionTransformer\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.has_mps:\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data \n",
    "#algorithm to read all the files\n",
    "\n",
    "'''\n",
    "for folder in this folder:\n",
    "    read xelasensor1.csv\n",
    "    read sliplabel.csv\n",
    "    concat it in a single dataframe along axis = 0\n",
    "\n",
    "print the dataframe\n",
    "'''\n",
    "\n",
    "directory = 'train2dof'\n",
    "directory2 = '/Users/elijahnelson/Desktop/SIWES/IML/Tactile_IML/'\n",
    "\n",
    "def read_file(detect_or_pred, n = None):\n",
    "\n",
    "    #store all directories in a list\n",
    "    list_xela_allfiles = []\n",
    "    list_sliplabel_allfiles = []\n",
    "\n",
    "    for root, subdirectories, files in os.walk(directory):\n",
    "        for sdirectory in subdirectories:\n",
    "\n",
    "            #subdirectory with absolute path\n",
    "            subdirectory = '{}/{}'.format(root, sdirectory)\n",
    "\n",
    "            #read specific files in the subdirectory\n",
    "            for file in os.listdir(subdirectory):\n",
    "            \n",
    "                if not file.startswith(\".\") and file.endswith(\"sensor1.csv\"):\n",
    "                    df = pd.read_csv(f'{subdirectory}/{file}', index_col=None, header=0)\n",
    "                    \n",
    "                    if detect_or_pred ==0:\n",
    "                        list_xela_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None:\n",
    "                        list_xela_allfiles.append(df[:-n])\n",
    "\n",
    "                if not file.startswith(\".\") and file.endswith(\"label.csv\"):\n",
    "                    df = pd.read_csv('{}/{}'.format(subdirectory, file), index_col=None, header=0)\n",
    "                    if detect_or_pred ==0:\n",
    "                        list_sliplabel_allfiles.append(df)\n",
    "                    elif detect_or_pred ==1 and n is not None: \n",
    "                        list_sliplabel_allfiles.append(df[n:])\n",
    "\n",
    "    return list_xela_allfiles, list_sliplabel_allfiles\n",
    "\n",
    "    #np.newaxis; np.zeros (3,4,4) -> \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the list of xela_allfiles and sliplabel_allfiles across axis = 0\n",
    "n = 0\n",
    "# n = 5\n",
    "list_xela_allfiles, list_sliplabel_allfiles = read_file(0)\n",
    "\n",
    "#for slip prediction, comment the line above and uncomment the line below\n",
    "# list_xela_allfiles, list_sliplabel_allfiles = read_file(1, n)\n",
    "\n",
    "pd_xela_allfiles = pd.concat(list_xela_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd.concat(list_sliplabel_allfiles, axis=0, ignore_index=True)\n",
    "pd_sliplabel_allfiles = pd_sliplabel_allfiles['slip']\n",
    "\n",
    "#reshape the target array into (rows, 1)\n",
    "tac_label = pd_sliplabel_allfiles.values.reshape(pd_sliplabel_allfiles.shape[0], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['txl1_x', 'txl1_y', 'txl1_z', 'txl2_x', 'txl2_y', 'txl2_z', 'txl3_x',\n",
       "       'txl3_y', 'txl3_z', 'txl4_x', 'txl4_y', 'txl4_z', 'txl5_x', 'txl5_y',\n",
       "       'txl5_z', 'txl6_x', 'txl6_y', 'txl6_z', 'txl7_x', 'txl7_y', 'txl7_z',\n",
       "       'txl8_x', 'txl8_y', 'txl8_z', 'txl9_x', 'txl9_y', 'txl9_z', 'txl10_x',\n",
       "       'txl10_y', 'txl10_z', 'txl11_x', 'txl11_y', 'txl11_z', 'txl12_x',\n",
       "       'txl12_y', 'txl12_z', 'txl13_x', 'txl13_y', 'txl13_z', 'txl14_x',\n",
       "       'txl14_y', 'txl14_z', 'txl15_x', 'txl15_y', 'txl15_z', 'txl16_x',\n",
       "       'txl16_y', 'txl16_z'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd_sliplabel_allfiles.to_csv('labels.csv')\n",
    "pd_xela_allfiles.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RE-ARRANGEMENT OF TABULAR DATA INTO IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrange the data by 3, 4, 4\n",
    "\n",
    "#arrange the columns by x, y, z\n",
    "col_x = []\n",
    "col_y = []\n",
    "col_z = []\n",
    "\n",
    "pd_columns = pd_xela_allfiles.columns\n",
    "for col in pd_columns:\n",
    "    if col.endswith('x'):\n",
    "        col_x.append(col)\n",
    "    \n",
    "    elif col.endswith('y'):\n",
    "        col_y.append(col)\n",
    "    \n",
    "    elif col.endswith('z'):\n",
    "        col_z.append(col)\n",
    "\n",
    "#arrange the table using the arranged columns\n",
    "pd_xela_allfiles_x = pd_xela_allfiles[col_x]\n",
    "pd_xela_allfiles_y = pd_xela_allfiles[col_y]\n",
    "pd_xela_allfiles_z = pd_xela_allfiles[col_z]\n",
    "\n",
    "\n",
    "#scale the data in the arranged columns\n",
    "#scale the data of the features\n",
    "\n",
    "sc = MinMaxScaler() #standard scaler\n",
    "sc.fit(pd_xela_allfiles_x)\n",
    "pd_xela_allfiles_x = sc.transform(pd_xela_allfiles_x)\n",
    "\n",
    "sc.fit(pd_xela_allfiles_y)\n",
    "pd_xela_allfiles_y = sc.transform(pd_xela_allfiles_y)\n",
    "\n",
    "sc.fit(pd_xela_allfiles_z)\n",
    "pd_xela_allfiles_z = sc.transform(pd_xela_allfiles_z)\n",
    "\n",
    "\n",
    "\n",
    "#reshape the arranged data per row to (4,4) AND rotate 90 degree anti-clockwise and append to a list\n",
    "pd_x = []\n",
    "pd_y = []\n",
    "pd_z = []\n",
    "\n",
    "for row in range(len(pd_xela_allfiles_x)):\n",
    "    pd_x.append(np.rot90(pd_xela_allfiles_x[row].reshape(4,4)))\n",
    "    pd_y.append(np.rot90(pd_xela_allfiles_y[row].reshape(4,4)))\n",
    "    pd_z.append(np.rot90(pd_xela_allfiles_z[row].reshape(4,4)))\n",
    "\n",
    "#add all the x, y, z in a single list\n",
    "pd_main = [pd_x, pd_y, pd_z]\n",
    "\n",
    "#arrange pd_main in a 3, 4, 4 array where its 3(4, 4) of x, y, z values\n",
    "pd_image = np.zeros( (pd_xela_allfiles.shape[0], 3, 4, 4))\n",
    "\n",
    "#per row, get (4,4) of x, y, z and assign it to pd_image to form the image\n",
    "for row in range(pd_xela_allfiles.shape[0]):\n",
    "    x_4_4 = pd_main[0][row]\n",
    "    y_4_4 = pd_main[1][row]\n",
    "    z_4_4 = pd_main[2][row]\n",
    "\n",
    "    pd_image[row][0] = x_4_4\n",
    "    pd_image[row][1] = y_4_4\n",
    "    pd_image[row][2] = z_4_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up_size = 224\n",
    "# n_images = len(pd_x)\n",
    "# tac_image = np.zeros((n_images, 3, up_size, up_size), np.float32) \n",
    "# for row in range(n_images):\n",
    "\n",
    "#     #resize image to 3, up_size, up_size\n",
    "#     for channel in range(3):\n",
    "#         image_per_channel = pd_image[row][channel]\n",
    "#         tac_image[row][channel] = cv2.resize(image_per_channel.astype(np.float32), dsize=(up_size, up_size), interpolation=cv2.INTER_CUBIC)\n",
    "# pd_image = tac_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_label = pd_sliplabel_allfiles.values.reshape(pd_sliplabel_allfiles.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([229651, 3, 4, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAGiCAYAAACcbHM0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAizUlEQVR4nO3df1BU973/8ddGYLEVNlUCYkXAm4Qo1lwLSdlM1KhTDEwd0/q9k0wzhLRJWjr+uEqd3mDSadI7GXI7NiWZJFBv/dHUpmamq6kZrVe+jYD9CokYvNqo1HsvCmXYUL1mMeS6iPl8/8i4czcsCJazrHyej5kzkz18zvr205pn9pfrMsYYAQAwzt001gMAABANBA8AYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAVHg3fhwgWVlJTI4/HI4/GopKREH3744ZDXPProo3K5XGFHQUGBk2MCACwQ5+Sdf/Ob39Rf/vIX7du3T5L0ne98RyUlJXrrrbeGvO7+++/X1q1bQ7cTEhKcHBMAYAHHgnfy5Ent27dPTU1N+spXviJJ+td//Vd5vV61trYqJydn0GvdbremTp3q1GgAAAs5FrzGxkZ5PJ5Q7CSpoKBAHo9Hhw4dGjJ4dXV1Sk1N1c0336yFCxfqueeeU2pqasS1wWBQwWAwdPuTTz7Rf//3f2vKlClyuVyj9xsCAESFMUYXL17UtGnTdNNNo/fKm2PB8/v9ESOVmpoqv98/6HVFRUX6h3/4B2VmZqqtrU0//OEPtXjxYh05ckRut3vA+srKSj377LOjOjsAYOx1dHRo+vTpo3Z/Iw7eM888c83AHD58WJIiPsIyxgz5yOvBBx8M/fOcOXOUn5+vzMxM7dmzR9/4xjcGrK+oqFB5eXnodiAQ0IwZM1S6d7kSPh9/zd8P/nbHK+eO9QjW6f6yoy+/4zMy9/WM9QhW6b8S1MF//5mSkpJG9X5H/Kdm1apVeuihh4Zck5WVpWPHjumDDz4Y8LO//vWvSktLG/avl56erszMTJ0+fTriz91ud8RHfgmfj1fCJIIXDXHxiWM9gnUmJBK8aIqb0DfWI1hptF+WGvGfmpSUFKWkpFxzndfrVSAQ0Lvvvqu7775bkvTOO+8oEAjonnvuGfavd/78eXV0dCg9PX2kowIAEOLY5/BmzZql+++/X0888YSamprU1NSkJ554Ql/72tfC3rByxx13aNeuXZKkjz76SOvXr1djY6POnDmjuro6LVu2TCkpKfr617/u1KgAAAs4+sHzX//61/rSl76kwsJCFRYWau7cufrVr34Vtqa1tVWBQECSNGHCBB0/flzLly/X7bffrtLSUt1+++1qbGwc9edyAQB2cfSFgMmTJ2v79u1DrjHGhP554sSJ+rd/+zcnRwIAWIq/SxMAYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFghKsF79dVXlZ2drcTEROXl5engwYNDrq+vr1deXp4SExM1c+ZM1dTURGNMAMA45njw3njjDa1du1ZPPfWUWlpaNH/+fBUVFam9vT3i+ra2NhUXF2v+/PlqaWnRhg0btGbNGvl8PqdHBQCMY44H74UXXtBjjz2mxx9/XLNmzVJVVZUyMjJUXV0dcX1NTY1mzJihqqoqzZo1S48//ri+/e1va+PGjU6PCgAYxxwNXl9fn44cOaLCwsKw84WFhTp06FDEaxobGwesX7p0qZqbm3X58uUB64PBoHp6esIOAAA+y9HgnTt3TleuXFFaWlrY+bS0NPn9/ojX+P3+iOv7+/t17ty5AesrKyvl8XhCR0ZGxuj9BgAA40ZU3rTicrnCbhtjBpy71vpI5yWpoqJCgUAgdHR0dIzCxACA8SbOyTtPSUnRhAkTBjya6+7uHvAo7qqpU6dGXB8XF6cpU6YMWO92u+V2u0dvaADAuOToI7yEhATl5eWptrY27Hxtba3uueeeiNd4vd4B6/fv36/8/HzFx8c7NisAYHxz/CnN8vJy/eIXv9CWLVt08uRJrVu3Tu3t7SorK5P06VOSjzzySGh9WVmZzp49q/Lycp08eVJbtmzR5s2btX79eqdHBQCMY44+pSlJDz74oM6fP68f//jH6urq0pw5c7R3715lZmZKkrq6usI+k5edna29e/dq3bp1euWVVzRt2jS99NJLWrFihdOjAgDGMZe5+o6QcaKnp0cej0dP1P8fJUziKdBo+Pdn5o31CNb54C7H/1sV/0vWW3zcKZr6r1zSgfeeVyAQUHJy8qjdL3+XJgDACgQPAGAFggcAsALBAwBYgeABAKxA8AAAViB4AAArEDwAgBUIHgDACgQPAGAFggcAsALBAwBYgeABAKxA8AAAViB4AAArEDwAgBUIHgDACgQPAGAFggcAsALBAwBYgeABAKxA8AAAViB4AAArEDwAgBUIHgDACgQPAGAFggcAsALBAwBYgeABAKxA8AAAViB4AAArEDwAgBUIHgDACgQPAGAFggcAsEJUgvfqq68qOztbiYmJysvL08GDBwddW1dXJ5fLNeA4depUNEYFAIxTjgfvjTfe0Nq1a/XUU0+ppaVF8+fPV1FRkdrb24e8rrW1VV1dXaHjtttuc3pUAMA45njwXnjhBT322GN6/PHHNWvWLFVVVSkjI0PV1dVDXpeamqqpU6eGjgkTJjg9KgBgHItz8s77+vp05MgRPfnkk2HnCwsLdejQoSGvnTdvni5duqTZs2fr6aef1qJFiyKuCwaDCgaDods9PT2SpJ+kv6fkJF6ijIZF/zhtrEewju+2HWM9glV+sO2hsR7BKuaT4LUXXQdHi3Du3DlduXJFaWlpYefT0tLk9/sjXpOenq5NmzbJ5/Np586dysnJ0ZIlS9TQ0BBxfWVlpTweT+jIyMgY9d8HAODG5+gjvKtcLlfYbWPMgHNX5eTkKCcnJ3Tb6/Wqo6NDGzdu1IIFCwasr6ioUHl5eeh2T08P0QMADODoI7yUlBRNmDBhwKO57u7uAY/6hlJQUKDTp09H/Jnb7VZycnLYAQDAZzkavISEBOXl5am2tjbsfG1tre65555h309LS4vS09NHezwAgEUcf0qzvLxcJSUlys/Pl9fr1aZNm9Te3q6ysjJJnz4l2dnZqddee02SVFVVpaysLOXm5qqvr0/bt2+Xz+eTz+dzelQAwDjmePAefPBBnT9/Xj/+8Y/V1dWlOXPmaO/evcrMzJQkdXV1hX0mr6+vT+vXr1dnZ6cmTpyo3Nxc7dmzR8XFxU6PCgAYx1zGGDPWQ4ymnp4eeTweXfjzTD6WECWL3l8+1iNY52U+lhBVP1jExxKiqf+ToP7vmZcVCARG9X0ZFAEAYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFiB4AEArOBo8BoaGrRs2TJNmzZNLpdLb7755jWvqa+vV15enhITEzVz5kzV1NQ4OSIAwBKOBq+3t1d33nmnXn755WGtb2trU3FxsebPn6+WlhZt2LBBa9askc/nc3JMAIAF4py886KiIhUVFQ17fU1NjWbMmKGqqipJ0qxZs9Tc3KyNGzdqxYoVEa8JBoMKBoOh2z09PX/TzACA8SmmXsNrbGxUYWFh2LmlS5equblZly9fjnhNZWWlPB5P6MjIyIjGqACAG0xMBc/v9ystLS3sXFpamvr7+3Xu3LmI11RUVCgQCISOjo6OaIwKALjBOPqU5vVwuVxht40xEc9f5Xa75Xa7HZ8LAHBji6lHeFOnTpXf7w87193drbi4OE2ZMmWMpgIAjAcxFTyv16va2tqwc/v371d+fr7i4+PHaCoAwHjgaPA++ugjHT16VEePHpX06ccOjh49qvb2dkmfvv72yCOPhNaXlZXp7NmzKi8v18mTJ7VlyxZt3rxZ69evd3JMAIAFHH0Nr7m5WYsWLQrdLi8vlySVlpZq27Zt6urqCsVPkrKzs7V3716tW7dOr7zyiqZNm6aXXnpp0I8kAAAwXI4G77777gu96SSSbdu2DTi3cOFCvffeew5OBQCwUUy9hgcAgFMIHgDACgQPAGAFggcAsALBAwBYgeABAKxA8AAAViB4AAArEDwAgBUIHgDACgQPAGAFggcAsALBAwBYgeABAKxA8AAAViB4AAArEDwAgBUIHgDACgQPAGAFggcAsALBAwBYgeABAKxA8AAAViB4AAArEDwAgBUIHgDACgQPAGAFggcAsALBAwBYgeABAKxA8AAAViB4AAArEDwAgBUIHgDACgQPAGAFR4PX0NCgZcuWadq0aXK5XHrzzTeHXF9XVyeXyzXgOHXqlJNjAgAsEOfknff29urOO+/Ut771La1YsWLY17W2tio5OTl0+5ZbbnFiPACARRwNXlFRkYqKikZ8XWpqqm6++eZhrQ0GgwoGg6HbPT09I/71AADjn6PBu17z5s3TpUuXNHv2bD399NNatGjRoGsrKyv17LPPDjjf2f+Revp5iTIaznamjPUI1ln257VjPYJVvjhvrCewS//lS9KZ0b/fmCpCenq6Nm3aJJ/Pp507dyonJ0dLlixRQ0PDoNdUVFQoEAiEjo6OjihODAC4UcTUI7ycnBzl5OSEbnu9XnV0dGjjxo1asGBBxGvcbrfcbne0RgQA3KBi6hFeJAUFBTp9+vRYjwEAuMHFfPBaWlqUnp4+1mMAAG5wjj6l+dFHH+k//uM/Qrfb2tp09OhRTZ48WTNmzFBFRYU6Ozv12muvSZKqqqqUlZWl3Nxc9fX1afv27fL5fPL5fE6OCQCwgKPBa25uDnuHZXl5uSSptLRU27ZtU1dXl9rb20M/7+vr0/r169XZ2amJEycqNzdXe/bsUXFxsZNjAgAs4DLGmLEeYjT19PTI4/HoTydSlZQU88/YjgsL3v7HsR7BPr0x9X6zce+LfxjrCezSf/mS3t39QwUCgbC/hORvRREAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVHA1eZWWl7rrrLiUlJSk1NVUPPPCAWltbr3ldfX298vLylJiYqJkzZ6qmpsbJMQEAFnA0ePX19Vq5cqWamppUW1ur/v5+FRYWqre3d9Br2traVFxcrPnz56ulpUUbNmzQmjVr5PP5nBwVADDOxTl55/v27Qu7vXXrVqWmpurIkSNasGBBxGtqamo0Y8YMVVVVSZJmzZql5uZmbdy4UStWrHByXADAOBbV1/ACgYAkafLkyYOuaWxsVGFhYdi5pUuXqrm5WZcvXx6wPhgMqqenJ+wAAOCzohY8Y4zKy8t17733as6cOYOu8/v9SktLCzuXlpam/v5+nTt3bsD6yspKeTye0JGRkTHqswMAbnxRC96qVat07Ngx/eY3v7nmWpfLFXbbGBPxvCRVVFQoEAiEjo6OjtEZGAAwrjj6Gt5Vq1ev1u7du9XQ0KDp06cPuXbq1Kny+/1h57q7uxUXF6cpU6YMWO92u+V2u0d1XgDA+OPoIzxjjFatWqWdO3fq7bffVnZ29jWv8Xq9qq2tDTu3f/9+5efnKz4+3qlRAQDjnKPBW7lypbZv367XX39dSUlJ8vv98vv9+p//+Z/QmoqKCj3yyCOh22VlZTp79qzKy8t18uRJbdmyRZs3b9b69eudHBUAMM45Grzq6moFAgHdd999Sk9PDx1vvPFGaE1XV5fa29tDt7Ozs7V3717V1dXp7//+7/XP//zPeumll/hIAgDgb+Loa3hX32wylG3btg04t3DhQr333nsOTAQAsBV/lyYAwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALCCo8GrrKzUXXfdpaSkJKWmpuqBBx5Qa2vrkNfU1dXJ5XINOE6dOuXkqACAcc7R4NXX12vlypVqampSbW2t+vv7VVhYqN7e3mte29raqq6urtBx2223OTkqAGCci3Pyzvft2xd2e+vWrUpNTdWRI0e0YMGCIa9NTU3VzTff7OB0AACbOBq8zwoEApKkyZMnX3PtvHnzdOnSJc2ePVtPP/20Fi1aFHFdMBhUMBgM3e7p6ZEkTbppgpJu4iXKaIifeHmsR7CO+5h7rEewyud2HhrrEazSb5z5d0rUimCMUXl5ue69917NmTNn0HXp6enatGmTfD6fdu7cqZycHC1ZskQNDQ0R11dWVsrj8YSOjIwMp34LAIAbmMsYY6LxC61cuVJ79uzRH//4R02fPn1E1y5btkwul0u7d+8e8LNIj/AyMjJ05lS6kpN4hBcNd/+/7471CNZxvzNprEewSvoLPMKLpn5zWXX6nQKBgJKTk0ftfqNShNWrV2v37t06cODAiGMnSQUFBTp9+nTEn7ndbiUnJ4cdAAB8lqOv4RljtHr1au3atUt1dXXKzs6+rvtpaWlRenr6KE8HALCJo8FbuXKlXn/9df3ud79TUlKS/H6/JMnj8WjixImSpIqKCnV2duq1116TJFVVVSkrK0u5ubnq6+vT9u3b5fP55PP5nBwVADDOORq86upqSdJ9990Xdn7r1q169NFHJUldXV1qb28P/ayvr0/r169XZ2enJk6cqNzcXO3Zs0fFxcVOjgoAGOei9qaVaOnp6ZHH4+FNK1HEm1aijzetRBdvWomuG/pNKwAAjDWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFiB4AEArEDwAABWIHgAACsQPACAFQgeAMAKBA8AYAWCBwCwAsEDAFjB0eBVV1dr7ty5Sk5OVnJysrxer37/+98PeU19fb3y8vKUmJiomTNnqqamxskRAQCWcDR406dP1/PPP6/m5mY1Nzdr8eLFWr58ud5///2I69va2lRcXKz58+erpaVFGzZs0Jo1a+Tz+ZwcEwBggTgn73zZsmVht5977jlVV1erqalJubm5A9bX1NRoxowZqqqqkiTNmjVLzc3N2rhxo1asWOHkqACAcS5qr+FduXJFO3bsUG9vr7xeb8Q1jY2NKiwsDDu3dOlSNTc36/LlyxGvCQaD6unpCTsAAPgsx4N3/PhxTZo0SW63W2VlZdq1a5dmz54dca3f71daWlrYubS0NPX39+vcuXMRr6msrJTH4wkdGRkZo/57AADc+BwPXk5Ojo4ePaqmpiZ973vfU2lpqU6cODHoepfLFXbbGBPx/FUVFRUKBAKho6OjY/SGBwCMG46+hidJCQkJuvXWWyVJ+fn5Onz4sF588UX9/Oc/H7B26tSp8vv9Yee6u7sVFxenKVOmRLx/t9stt9s9+oMDAMaVqH8OzxijYDAY8Wder1e1tbVh5/bv36/8/HzFx8dHYzwAwDjlaPA2bNiggwcP6syZMzp+/Lieeuop1dXV6eGHH5b06dORjzzySGh9WVmZzp49q/Lycp08eVJbtmzR5s2btX79eifHBABYwNGnND/44AOVlJSoq6tLHo9Hc+fO1b59+/TVr35VktTV1aX29vbQ+uzsbO3du1fr1q3TK6+8omnTpumll17iIwkAgL+Zo8HbvHnzkD/ftm3bgHMLFy7Ue++959BEAABb8XdpAgCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVCB4AwAoEDwBgBYIHALACwQMAWIHgAQCsQPAAAFYgeAAAKxA8AIAVHA1edXW15s6dq+TkZCUnJ8vr9er3v//9oOvr6urkcrkGHKdOnXJyTACABeKcvPPp06fr+eef16233ipJ+uUvf6nly5erpaVFubm5g17X2tqq5OTk0O1bbrnFyTEBABZwNHjLli0Lu/3cc8+purpaTU1NQwYvNTVVN99887B+jWAwqGAwGLodCAQkSRc/+mTkA+O6fPLxpbEewTpXgo7+0cVn9JvLYz2CVfr16X4bY0b3jk2U9Pf3m9/85jcmISHBvP/++xHXHDhwwEgyWVlZZurUqWbx4sXm7bffHvJ+f/SjHxlJHBwcHBzj7PjP//zPUe2Qy5jRTmi448ePy+v16tKlS5o0aZJef/11FRcXR1zb2tqqhoYG5eXlKRgM6le/+pVqampUV1enBQsWRLzms4/wPvzwQ2VmZqq9vV0ej8eR35NTenp6lJGRoY6OjrCndGMdc0cXc0ffjTr7jTp3IBDQjBkzdOHChWE/2zccjj8vkpOTo6NHj+rDDz+Uz+dTaWmp6uvrNXv27Ihrc3JyQre9Xq86Ojq0cePGQYPndrvldrsHnPd4PDfU/8D/29U3+dxomDu6mDv6btTZb9S5b7ppdN9X6fjHEhISEnTrrbcqPz9flZWVuvPOO/Xiiy8O+/qCggKdPn3awQkBADaI+ufwjDFhT0FeS0tLi9LT0x2cCABgA0ef0tywYYOKioqUkZGhixcvaseOHaqrq9O+ffskSRUVFers7NRrr70mSaqqqlJWVpZyc3PV19en7du3y+fzyefzDfvXdLvd+tGPfhTxac5Yd6POztzRxdzRd6POztzhHH3TymOPPaY//OEP6urqksfj0dy5c/VP//RP+upXvypJevTRR3XmzBnV1dVJkn7yk59o06ZN6uzs1MSJE5Wbm6uKiopB3+QCAMBwOf4uTQAAYgF/lyYAwAoEDwBgBYIHALACwQMAWGFcBO/ChQsqKSmRx+ORx+NRSUmJPvzwwyGvefTRRwd8DVFBQYGjc7766qvKzs5WYmKi8vLydPDgwSHX19fXKy8vT4mJiZo5c6ZqamocnW8oI5k9Vr7mqaGhQcuWLdO0adPkcrn05ptvXvOaWNjzkc4dC/tdWVmpu+66S0lJSUpNTdUDDzyg1tbWa14XC/t9PbPHwp6P9OvXpNjY77H82rhxEbxvfvObOnr0qPbt26d9+/bp6NGjKikpueZ1999/v7q6ukLH3r17HZvxjTfe0Nq1a/XUU0+ppaVF8+fPV1FRkdrb2yOub2trU3FxsebPn6+WlhZt2LBBa9asGdFnEkfLSGe/qrW1NWx/b7vttihN/Kne3l7deeedevnll4e1Plb2fKRzXzWW+11fX6+VK1eqqalJtbW16u/vV2FhoXp7ewe9Jlb2+3pmv2os9/zq1681NzerublZixcv1vLly/X+++9HXB8r+z3Sua8alb0e1b+KegycOHHCSDJNTU2hc42NjUaSOXXq1KDXlZaWmuXLl0dhwk/dfffdpqysLOzcHXfcYZ588smI63/wgx+YO+64I+zcd7/7XVNQUODYjIMZ6exXv/XiwoULUZhueCSZXbt2Dbkmlvb8quHMHYv73d3dbSSZ+vr6QdfE4n4bM7zZY3HPjTHmC1/4gvnFL34R8Wexut/GDD33aO71Df8Ir7GxUR6PR1/5yldC5woKCuTxeHTo0KEhr62rq1Nqaqpuv/12PfHEE+ru7nZkxr6+Ph05ckSFhYVh5wsLCwedsbGxccD6pUuXqrm5WZcvR++7ua5n9qvmzZun9PR0LVmyRAcOHHByzFERK3t+vWJpv69+L+XkyZMHXROr+z2c2a+KlT2/cuWKduzYod7eXnm93ohrYnG/hzP3VaOx1zd88Px+v1JTUwecT01Nld/vH/S6oqIi/frXv9bbb7+tn/70pzp8+LAWL148or/nc7jOnTunK1euKC0tLex8WlraoDP6/f6I6/v7+3Xu3LlRn3Ew1zN7enq6Nm3aJJ/Pp507dyonJ0dLlixRQ0NDNEa+brGy5yMVa/ttjFF5ebnuvfdezZkzZ9B1sbjfw509Vvb8+PHjmjRpktxut8rKyrRr166I30QjxdZ+j2Tu0dzrmP3a5GeeeUbPPvvskGsOHz4sSXK5XAN+ZoyJeP6qBx98MPTPc+bMUX5+vjIzM7Vnzx594xvfuM6ph/bZea41Y6T1kc5Hw0hmv56veYoVsbTnwxVr+71q1SodO3ZMf/zjH6+5Ntb2e7izx8qej+Tr16TY2W+nvzZuMDEbvFWrVumhhx4ack1WVpaOHTumDz74YMDP/vrXvw74r5mhpKenKzMz05GvIkpJSdGECRMGPCLq7u4edMapU6dGXB8XF6cpU6aM+oyDuZ7ZIykoKND27dtHe7xRFSt7PhrGar9Xr16t3bt3q6GhQdOnTx9ybazt90hmj2Qs9vzq169JUn5+vg4fPqwXX3xRP//5zwesjaX9HsnckVzvXsds8FJSUpSSknLNdV6vV4FAQO+++67uvvtuSdI777yjQCCge+65Z9i/3vnz59XR0eHIVxElJCQoLy9PtbW1+vrXvx46X1tbq+XLl0e8xuv16q233go7t3//fuXn5ys+Pn7UZxzM9cweyY3wNU+xsuejIdr7bYzR6tWrtWvXLtXV1Sk7O/ua18TKfl/P7JHEwv/HzRBfvxYr+x3JUHNHct17/Te/7SUG3H///Wbu3LmmsbHRNDY2mi996Uvma1/7WtianJwcs3PnTmOMMRcvXjTf//73zaFDh0xbW5s5cOCA8Xq95otf/KLp6elxZMYdO3aY+Ph4s3nzZnPixAmzdu1a8/nPf96cOXPGGGPMk08+aUpKSkLr/+u//st87nOfM+vWrTMnTpwwmzdvNvHx8ea3v/2tI/ON5uw/+9nPzK5du8yf//xn86c//ck8+eSTRpLx+XxRnfvixYumpaXFtLS0GEnmhRdeMC0tLebs2bMR546VPR/p3LGw39/73veMx+MxdXV1pqurK3R8/PHHoTWxut/XM3ss7HlFRYVpaGgwbW1t5tixY2bDhg3mpptuMvv37484c6zs90jnHs29HhfBO3/+vHn44YdNUlKSSUpKMg8//PCAt7BKMlu3bjXGGPPxxx+bwsJCc8stt5j4+HgzY8YMU1paatrb2x2d85VXXjGZmZkmISHBfPnLXw5723NpaalZuHBh2Pq6ujozb948k5CQYLKyskx1dbWj8w1lJLP/y7/8i/m7v/s7k5iYaL7whS+Ye++91+zZsyfqM199O/Nnj9LS0ohzGxMbez7SuWNhvyPN+7//zEWa25jY2O/rmT0W9vzb3/526M/kLbfcYpYsWRKKRqSZjYmN/R7p3KO513w9EADACjf8xxIAABgOggcAsALBAwBYgeABAKxA8AAAViB4AAArEDwAgBUIHgDACgQPAGAFggcAsALBAwBY4f8DttBXGEW7GM4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd_image.shape)\n",
    "plt.imshow(pd_image[0][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_image = torch.from_numpy(pd_image.astype(np.float32))\n",
    "pd_label = torch.from_numpy(pd_label.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timii/miniconda3/envs/aienv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7834, 0.7834, 0.7834,  ..., 0.6412, 0.6412, 0.6412],\n",
       "         [0.7834, 0.7834, 0.7834,  ..., 0.6412, 0.6412, 0.6412],\n",
       "         [0.7834, 0.7834, 0.7834,  ..., 0.6412, 0.6412, 0.6412],\n",
       "         ...,\n",
       "         [0.7064, 0.7064, 0.7064,  ..., 0.4572, 0.4572, 0.4572],\n",
       "         [0.7064, 0.7064, 0.7064,  ..., 0.4572, 0.4572, 0.4572],\n",
       "         [0.7064, 0.7064, 0.7064,  ..., 0.4572, 0.4572, 0.4572]],\n",
       "\n",
       "        [[0.6125, 0.6125, 0.6125,  ..., 0.2897, 0.2897, 0.2897],\n",
       "         [0.6125, 0.6125, 0.6125,  ..., 0.2897, 0.2897, 0.2897],\n",
       "         [0.6125, 0.6125, 0.6125,  ..., 0.2897, 0.2897, 0.2897],\n",
       "         ...,\n",
       "         [0.7159, 0.7159, 0.7159,  ..., 0.2103, 0.2103, 0.2103],\n",
       "         [0.7159, 0.7159, 0.7159,  ..., 0.2103, 0.2103, 0.2103],\n",
       "         [0.7159, 0.7159, 0.7159,  ..., 0.2103, 0.2103, 0.2103]],\n",
       "\n",
       "        [[0.2512, 0.2512, 0.2512,  ..., 0.5122, 0.5122, 0.5122],\n",
       "         [0.2512, 0.2512, 0.2512,  ..., 0.5122, 0.5122, 0.5122],\n",
       "         [0.2512, 0.2512, 0.2512,  ..., 0.5122, 0.5122, 0.5122],\n",
       "         ...,\n",
       "         [0.1531, 0.1531, 0.1531,  ..., 0.2796, 0.2796, 0.2796],\n",
       "         [0.1531, 0.1531, 0.1531,  ..., 0.2796, 0.2796, 0.2796],\n",
       "         [0.1531, 0.1531, 0.1531,  ..., 0.2796, 0.2796, 0.2796]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_image_train, pd_image_test, pd_label_train, pd_label_test = train_test_split(pd_image, pd_label, test_size=0.1, shuffle=True)\n",
    "\n",
    "#split into train and validation\n",
    "pd_image_train, pd_image_valid, pd_label_train, pd_label_valid = train_test_split(pd_image_train, pd_label_train, test_size=0.3, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([144679, 3, 4, 4]), torch.Size([144679, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_image_train.shape, pd_label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq_dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.X.__len__()\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].numpy(), self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = seq_dataset(pd_image_train, pd_label_train)\n",
    "valid_dataset = seq_dataset(pd_image_valid, pd_label_valid)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4096, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 3, 4, 4]) torch.Size([4096, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    x_try = i[0]\n",
    "    print(i[0].shape, i[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "ViT_model_class_1000 = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViTModel, self).__init__()\n",
    "        self.ViT_model_class = ViT_model_class_1000\n",
    "        self.linear = nn.Linear(1000, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.ViT_model_class(x)\n",
    "        x = self.linear(x[0])\n",
    "        return x\n",
    "    \n",
    "model = ViTModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device))\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (1418485841.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    for (image_transform(x), y) in (train_loader):\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "#Training and validation loop \n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "\n",
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Train per batch\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "\n",
    "    model.train()\n",
    "    for (x, y) in (train_loader):\n",
    "        #Forward pass\n",
    "        y_pred = model(x.to(device))\n",
    "        #compute the loss\n",
    "        l = criterion(y_pred.to(device), y.to(device))\n",
    "        #empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "        #compute the gradient\n",
    "        l.backward()\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "        #append each loss per batch\n",
    "        train_loss.append(l.item())\n",
    "        train_accuracy.append(accuracy_score(y.detach().cpu().numpy().round(), y_pred.detach().cpu().numpy().round()))\n",
    "        \n",
    "    \n",
    "\n",
    "    #calculate and plot the validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in (valid_loader):\n",
    "            y_pred = model(x.to(device))\n",
    "            lv = criterion(y_pred.to(device), y.to(device))\n",
    "            #append the loss per batch\n",
    "            valid_loss.append(lv.item())\n",
    "            #accuracy\n",
    "            valid_accuracy.append(accuracy_score(y.detach().cpu().numpy().round(), y_pred.detach().cpu().numpy().round()))\n",
    "\n",
    "    #append the total loss and accuracy per epoch\n",
    "    t_loss.append(np.mean(train_loss))\n",
    "    v_loss.append(np.mean(valid_loss))\n",
    "    t_acc.append(np.mean(train_accuracy))\n",
    "    v_acc.append(np.mean(valid_accuracy))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, loss = {np.mean(train_loss):.4f} , val_loss = {np.mean(valid_loss):.4f}')\n",
    "    torch.save(model.state_dict(), 'ViT_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'ViT_n{n}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(metric, title):\n",
    "    plt.plot(metric)\n",
    "    plt.title(title)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(t_loss, \"Train Loss\")\n",
    "plot(v_loss, \"Val Loss\")\n",
    "\n",
    "plot(t_acc, \"Train Accuracy\")\n",
    "plot(v_acc, \"Val Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "947f030b3e678118fc438144c1e47ca5c23949e6feee86165ca58c1240ce2eba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
